% -*- root: Main.tex -*-
\section{Classification}
\textbf{Loss-Functions}:
$y \in \{-1,1\}$, pred. $z \in \{-1,1\}$\\
Cross-entropy (log loss):\\
$-[y'\log(z') {+} (1-y')\log(1-z')]$\\ 
($y'=\tfrac{(1+y)}{2}$ and $z'=\tfrac{(1+z)}{2}$) \\
Hinge Loss: $max(0, 1-yz)$ \\
Perceptron Loss: $ max(0, -yz)$ \\
Logistic loss: $\log(1 + exp(-yz))$ \\
Square loss: $\tfrac{1}{2}(1-yz)^2$ \\
Exponential loss: $exp(-yz)$ \\
Binomial deviance: $1 + exp(-2yz)$ \\
0/1 Loss: $\mathbb{I}\{sign(z)\neq y\}$

\textbf{Metrics: ROC(FPR/TPR)}:\\
Accuracy: $\frac{TP+TN}{TP+TN+FP+FN}$, Precision: $\frac{TP}{TP+FP}$\\ Recall/TPR: $\frac{TP}{TP+FN}$, F1 score: $\frac{2TP}{2TP+FP+FN}$\\
Balanced accuracy: $\frac{1}{n}\sum_i^n TPR_i$
