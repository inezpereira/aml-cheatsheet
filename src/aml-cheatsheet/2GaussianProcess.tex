% -*- root: Main.tex -*-
\section{Gaussian Processes}
% A GP $\{X_t\}_t$ is a collection of random variables from which any finite sample has a joint Gaussian distribution.\\
% For any finite set of points $T=\{t_1, \dots, t_n\}$ from a GP, it hold that $(X_{t_1}, \dots,X_{t_n})\sim \mathcal{N}(\pmb{\mu_T},\pmb{\Sigma_T})$ with $\pmb{\mu_T} = (\mu(t_1),\dots,\mu(t_n))$, $\pmb{\Sigma_T}(i,j)=k(X_{t_i},X_{t_j})$
\subsection*{Gaussian Process}
%$p\big(\begin{bmatrix}
%\mathbf{y} \\
%y^*\\
%\end{bmatrix}|x^*,\mathbf{X}, \sigma \big) = \mathcal{N}\big(\begin{bmatrix}
%\mathbf{y} \\
%y^*\\
%\end{bmatrix} | \mathbf{0},\begin{bmatrix}
%\mathbf{C_n} & \mathbf{k} \\
%\mathbf{k}^T & c 
%\end{bmatrix}  \big)$\\
%with $\mathbf{C_n} = \mathbf{K} + \sigma^2 \mathbf{I}, c = k(x_{n+1},x_{n+1}) + \sigma^2,\\
%\mathbf{k}=k(x_{n+1}, \mathbf{X}), \mathbf{K}=k(\mathbf{X}, \mathbf{X})$\\
%$p(y^*|x^*, X, y) = \mathcal{N}(y^*|\mu, \sigma^2)$\\
%with $\mu = k^T C_n^{-1}y, \sigma^2 = c-k^TC_n^{-1}k$\\

$[y_1, y_2, ...]^T\!=\!X\beta\!+\!\epsilon \sim \mathcal{N}(y|0,X \Lambda^{-1} X^T+ \sigma^2 I) $
$y \sim \mathcal{N}(y | m(X), K(X,X) + \sigma^2 I) = P(y|X,\Theta)$ 

%Joint dist.: {\footnotesize $p([y, y_{n+1}] | x_{n+1}, X, \sigma) 
%\sim \mathcal{N}([y, y_{n+1}]|, K_{n+1} + \sigma^2I)$}
                    
$\left[\begin{smallmatrix} y \\y_{n+1}\end{smallmatrix}\right] \sim \mathcal{N}\left(\left[\begin{smallmatrix} y \\y_{n+1}\end{smallmatrix}\right]|[\begin{smallmatrix} m(X) \\m(x_{n+1})\end{smallmatrix}], [\begin{smallmatrix} C_n & k \\ k^T & c\end{smallmatrix}]\right)$
	
$p(y_{n+1}|x_{n+1}, X, y)) = \mathcal{N}(y_{n+1} | \mu_{n+1}, \sigma^2_{n+1})$	\\
$\mu_{n+1} = m(x_{n+1})+k^T C^{-1}_n (y\!-\!m(X))$ \\
$\sigma^2_{n+1} = c - k^T C^{-1}_n k$,$k = k(x_{n+1}, X)$ \\
$c = k(x_{n+1},x_{n+1})\!+\!\sigma^2$,$C_n = K_n + \sigma^2 I$

\subsection*{GP Hyperparameter Optimization}
Log-likelihood:\\
$l(Y|\theta) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |C_n| - \frac{1}{2} Y^T C_n^{-1}Y$\\
Set of hyperparameters $\theta$ determine parameters $C_n$. Gradient descent: $\nabla_{\theta_i}l(Y|\theta) = -\frac{1}{2}tr(C_n^{-1} \frac{\partial C_n}{\partial \theta_i}) + \frac{1}{2} Y^T C_n^{-1} \frac{\partial C_n}{\partial \theta_i} C_n^{-1} Y$

\subsection*{Kernels}

    Kernel $k(\b{x}, \b{x}')$ must be \textbf{symmetric} and \textbf{positive semi-definite}: for any $n \in \N$, $S = \set{\b{x}_1, \ldots, \b{x}_N}$, Gram matrix $\b{K}$, $K_{ij} \defeq k(\b{x}_i, \b{x}_j)$ must be PSD, i.e. $\b{x}^T\b{K}\b{x} \geq 0$ for all $\b{x}$ (All principal minors of $K$ need $det \geq 0$). (Gen.: \textit{Mercer's Thm}).

    % \textbf{Mercer's theorem}: if $K(x,y)=K(y,x)$ and $\int\int f(x)K(x,y)f(x)dx dy\geq0$, then $K$ is a kernel. Valid $\forall f$ where $\int f^2(x)dx<\infty$.

    For a valid kernel $k$, there must exist a (potentially $\infty$-dim.) feature vector $\b{\phi}(\b{x})$ s.t. $k(\b{x}, \b{x}') = \inner{\b{\phi}(\b{x})}{\b{\phi}(\b{x}')}$.
    
    \begin{tabular}{ll}
    \hline
    \textbf{Name} & $k(\b{x}, \b{x}') = $ \\
    \hline
    Linear & $\b{x}^T \b{x}'$ \\
    Polynomial & $(\b{x}^T\b{x'} + 1)^p$, where $p \in \N$ \\
    RBF (Gaussian) & $\exp(-\norm{\b{x} - \b{x'}}_2^2 / h^2)$ \\
    Sigmoid & $\tanh(\b{x}^T \b{x}') - b$ 
    \\
    \hline
    \end{tabular}

    \textbf{Kernel construction.} If $k_1$ and $k_2$ are valid kernels, these are also valid:
    $ck_1$ where $c > 0$;
    $f(\b{x}) k_1(\b{x}, \b{x'}) f(\b{x'})$;
    $k_1 + k_2$; $k_1 \cdot k_2$;
    $k(\phi(\b{x}), \phi(\b{x}'))$ where $\phi \colon \mathcal{X} \rightarrow \R^d$;
    $g(k_1(\b{x}, \b{x'}))$ where $g$ is a polynomial with positive coefs or the $\exp$ function.

	

