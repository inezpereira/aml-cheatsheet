\section{Neural Network}
\subsection*{Backpropagation}
For each unit $j$ on the output layer:\\
- Compute error signal: $\delta_j = \ell_j'(f_j)$\\
- For each unit $i$ on layer $L$: $\frac{\partial}{\partial w_{j,i}} = \delta_j v_i$

For each unit $j$ on hidden layer $l=\{L-1,..,1\}$:\\
- Error signal: $\delta_j = \phi'(z_j) \sum_{i\in Layer_{l+1}} w_{i,j}\delta_i$\\
- For each unit $i$ on layer $l-1$: $\frac{\partial}{\partial w_{j,i}} = \delta_j v_i$

\textbf{VAE}: define prior over $\mathcal{Z}$, likelihood (decoder) and approx. posterior (encoder). Requirements for good representation:\\
\begin{inparaitem}[\textbullet]
    \item \textbf{informative}: given $\mathcal{Z}$, should be easy to guess $\mathcal{X}$). Encoder function maximizes mutual information: $\theta^* = \argmax_\theta I(\mathcal{X};\mathcal{Z})=\argmax_\theta \E_{\mathcal{X},\mathcal{Z}}[\log\frac{P(\mathcal{X,\mathcal{Z}})}{P(\mathcal{X})P(\mathcal{Z})}]\\
    \approx \argmax_\theta \sum_{i\leq n} \E_{\mathcal{Z}|x_i}[\log P(x_i|\mathcal{Z})]$\\
    \item \textbf{disentangled}: components in $\mathcal{Z}$ associated with distinct feature in $\mathcal{X}$ (see $D_{KL}$ of ELBO). \\
    \item \textbf{robust}: noise in $\mathcal{X}$ does not substantially affect $\mathcal{Z}$ (vice versa). Choice of approx. post.!\\
\end{inparaitem}

\textbf{Training a VAE}: $\argmax_{\theta,\phi}\sum_{i\leqn}\log p_{\theta}(x_i)$\\
\textbf{Derivation of ELBO}:
$
    \log p_{\theta, \phi}(x_i)
    = \E_{Z \sim q_{\phi}(\cdot \mid x_i)}
    \bigg[ \log \Big(
    \frac{p_\theta(x_i, Z)}{p_\theta (Z \mid x_i)}
    \frac{q_\phi(Z \mid x_i)}{q_\phi(Z \mid x_i)}
    \Big) \bigg]
    =
    \underbrace{\E_{\mathcal{Z} \sim q_{\phi}(\cdot \mid x_i) } \big[
        \log p_{\theta}(x_i \mid Z)
    \big]
    - D_{KL} \big(
        q_\phi(\cdot \mid x_i)\ ||\ p(\cdot)
    \big)}_\text{ELBO = Infomax - Regularization term}
    + D_{KL} \big(
        q_\phi(\cdot \mid x_i)\ ||\ p_\theta(\cdot \mid x_i)
    \big)
$


