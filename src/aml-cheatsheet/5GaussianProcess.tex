% -*- root: Main.tex -*-
\section{Gaussian Processes}

\subsection*{Bayesian linear regression}
	\textbf{Model}:  \= $y = X^T \beta + \epsilon$, with $\epsilon \sim
	\mathcal{N}(\epsilon | 0, \sigma^2 I)$\\ 
	Likelihood $P(y | X, \beta, \sigma) = \mathcal{N}(y | X^T \beta , \sigma^2 I)$\\
	Prior (ridge regr. if $\Lambda = \lambda\mathbf{I}$ and $\sigma=1$) $P(\beta | \Lambda) = \mathcal{N} (\beta | 0, \Lambda^{-1})$\\ Posterior $P(\beta | X, y, \Lambda) = \mathcal{N}(\beta | \mu_\beta, \Sigma_\beta)$ with \\
	$\mu_\beta = (X^T X + \sigma^2 \Lambda)^{-1} X^T y$ and \\
	$\Sigma_\beta = \sigma^2(X^T X + \sigma^2 \Lambda)^{-1}$ \\

\subsection*{Gaussian Process}
    % $[y_1, y_2, ...]^T\!=\!X\beta\!+\!\epsilon \sim \mathcal{N}(y|0,\underbrace{X \Lambda^{-1} X^T}_\text{can kernelize}+ \sigma^2 I) $
    $y \sim \mathcal{N}(y | m(X), K(X,X) + \sigma^2 I) = P(y|X,\Theta)$ 
    
    %Joint dist.: {\footnotesize $p([y, y_{n+1}] | x_{n+1}, X, \sigma) 
    %\sim \mathcal{N}([y, y_{n+1}]|, K_{n+1} + \sigma^2I)$}
                        
    $$\left[\begin{smallmatrix} y \\y_{n+1}\end{smallmatrix}\right] \sim \mathcal{N}\left(\left[\begin{smallmatrix} y \\y_{n+1}\end{smallmatrix}\right]|[\begin{smallmatrix} m(X) \\m(x_{n+1})\end{smallmatrix}], [\begin{smallmatrix} C_n & k \\ k^T & c\end{smallmatrix}]\right)$$
    	
    $p(y_{n+1}|x_{n+1}, X, y)) = \mathcal{N}(y_{n+1} | \mu_{n+1}, \sigma^2_{n+1})$	\\
    $\mu_{n+1} = m(x_{n+1})+k^T C^{-1}_n (y\!-\!m(X))$ \\
    $\sigma^2_{n+1} = c - k^T C^{-1}_n k$, where $C_n = K(X,X) + \sigma^2 I$, $c = k(x_{n+1},x_{n+1})\!+\!\sigma^2$, $k = k(x_{n+1}, X)$ \\

\subsection*{Kernels}

    Kernel $k(\b{x}, \b{x}')$ must be \textbf{symmetric} and \textbf{positive semi-definite}: for any $n \in \N$, $S = \set{\b{x}_1, \ldots, \b{x}_N}$, Gram matrix $\b{K}$, $K_{ij} \defeq k(\b{x}_i, \b{x}_j)$ must be PSD, i.e. $\b{x}^T\b{K}\b{x} \geq 0$ for all $\b{x}$ (All principal minors of $K$ need $det \geq 0$). (Gen.: \textit{Mercer's Thm}).

    % \textbf{Mercer's theorem}: if $K(x,y)=K(y,x)$ and $\int\int f(x)K(x,y)f(x)dx dy\geq0$, then $K$ is a kernel. Valid $\forall f$ where $\int f^2(x)dx<\infty$.

    For a valid kernel $k$, there must exist a (potentially $\infty$-dim.) feature vector $\b{\phi}(\b{x})$ s.t. $k(\b{x}, \b{x}') = \inner{\b{\phi}(\b{x})}{\b{\phi}(\b{x}')}$.
    
    \begin{tabular}{ll}
    \hline
    \textbf{Name} & $k(\b{x}, \b{x}') = $ \\
    \hline
    Linear & $\b{x}^T \b{x}'$ \\
    Polynomial & $(\b{x}^T\b{x'} + 1)^p$, where $p \in \N$ \\
    RBF (Gaussian) & $\exp(-\norm{\b{x} - \b{x'}}_2^2 / h^2)$ \\
    Sigmoid & $\tanh(\b{x}^T \b{x}') - b$ 
    \\
    \hline
    \end{tabular}

    \textbf{Kernel construction.} If $k_1$ and $k_2$ are valid kernels, these are also valid:
    $ck_1$ where $c > 0$;
    $f(\b{x}) k_1(\b{x}, \b{x'}) f(\b{x'})$;
    $k_1 + k_2$; $k_1 \cdot k_2$;
    $k(\phi(\b{x}), \phi(\b{x}'))$ where $\phi \colon \mathcal{X} \rightarrow \R^d$;
    $g(k_1(\b{x}, \b{x'}))$ where $g$ is a polynomial with positive coefs or the $\exp$ function.

	

