% -*- root: Main.tex -*-
\section{Classification}
\subsection*{Loss-Functions}
True class: $y \in \{-1,1\}$, pred. $z \in [-1,1]$\\
Cross-entropy (log loss): ($y'=\tfrac{(1+y)}{2}$ and $z'=\tfrac{(1+z)}{2}$) $L(y',z') {=} -[y'log(z') {+} (1-y')log(1-z')]$ \\
Hinge Loss: $L(y,z) = max(0, 1-yz)$ \\
Perceptron Loss: $L(y,z) = max(0, -yz)$ \\
Logistic loss: $L(y,z) = log(1 + exp(-yz))$ \\
Square loss: $L(y,z) = \tfrac{1}{2}(1-yz)^2$ \\
Exponential loss: $L(y,z) = exp(-\beta yz)$ \\
Binomial deviance: $L(y,z) = 1 + exp(-2yz)$ \\
0/1 Loss: $L(y,z) = \mathbb{I}\{sign(z)\neq y\}$ \\

\hl{Possibly repeated:} \textbf{Cross-entropy (logistic) loss}, $y \in \set{0, 1}$: $-(y\log(\hat{y}) + (1-y) \log(1-\hat{y}))$

\textbf{Hinge loss}, $y \in \set{-1, 1}$: $\max(0, 1 - y \hat{y})$

\textbf{Exponential loss}, $y \in \set{-1, 1}$: $e^{-y\hat{y}}$



\subsection*{Metrics: ROC(FPR/TPR), PRC(Recall/Precision)}
Accuracy: $\frac{TP+TN}{TP+TN+FP+FN}$, Precision: $\frac{TP}{TP+FP}$\\ Recall/TPR: $\frac{TP}{TP+FN}$, F1 score: $\frac{2TP}{2TP+FP+FN}$\\
Balanced accuracy: $\frac{1}{n}\sum_i^n TPR_i$
