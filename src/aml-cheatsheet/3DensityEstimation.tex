\section{Density Estimation}

\subsection*{Bayesianism/Frequentism}

\textbf{Bayesian}: Define prior $P(\theta)$,
define likelihood $P(y_{1 \ldots n} \mid \theta)$,
compute posterior $P(\theta \mid y_{1 \ldots n})$
(Bayes' Rule, computing $P(data) = \sum_{\theta} P(data \mid \theta) P(\theta)$).

\textbf{Frequentist} (MLE):
Define a parametric model $\theta$ (e.g. $\mathcal{N}(\theta, 1)$),
compute likelihood of data
and compute MLE $\hat{\theta}_{ML} = \argmax_{\theta} P(y_{1 \ldots n} \mid \theta)$.


\subsection*{Estimation - Properties of ML-estimators:}
\textbf{Consistency}: $\forall\epsilon>0, P \{|\hat{\theta}-\theta_0| \geq\epsilon\} \stackrel{\tiny n \to\infty}{\longrightarrow} 0 $\\
\textbf{Equivariance}: If $\hat \theta_n$ is MLE of $\theta$, then $g(\hat \theta_n)$ is MLE of $g(\theta)$.\\
\textbf{Asympt. normality}:\\
$\sqrt{N}(\hat{\theta} - \theta_0) \to \mathcal{N}(0, J^{-1}(\theta_0)I(\theta_0)J^{-1}(\theta_0))$,\\ 
$J= -\E[\frac{\partial^2 \log \mathbb{P}(x|\theta) }{\partial \theta \partial \theta ^{T}}]$ and $I(\theta_0)=$ Fisher info \\
\textbf{Asympt. efficiency}: $\hat{\theta}$ minimizes $\E[(\hat{\theta}-\theta_0)^2]$ as $n \to\infty$ or $\E[(\hat{\theta}-\theta_0)^2] = \frac{1}{I_n(\theta)}$ (Rao Cramer)\\
$\hat{\theta}$ has smallest variance among all consistent estimators (for large enough N), i.e. $\lim_{n\to\infty} (\mathbb{V}[\hat{\theta}]I_n(\theta))^{-1} = 1$\\
% $\hat{\theta}_{MAP} := \argmax_\theta \left \{ \sum_{i=1}^n log(p(x_i | \theta) + log(p(\theta)) \right\}$
(Bias: $\text{bias}(\hat{\theta}) = \E[\hat{\theta}]-\theta$ = 0 if unbiased)

\textbf{Stein estimator}: $\hat{\theta}_{JS}$ better than $\hat{\theta}_{MLE}$ for finite samples, i.e. $\E[(\hat{\theta}_{JS}-\theta_0)^2]\leq \E[(\hat{\theta}_{MLE}-\theta_0)^2]$ where $\hat{\theta}_{JS}=\big(1-\frac{(d-2)\sigma^2}{||y||^2}\big)y, d\geq 3$. This shows that there exist biased estimators that are better in the least squares sense then any u-estimator.\\

\subsection*{Rao-Cramer}
$\Lambda = \frac{\partial \log \mathbb{P}(x|\theta )}{\partial \theta}=\frac{\partial \log \mathbb{P}(x_1,...x_n|\theta )}{\partial \theta}$ (score function), $\E[\Lambda ]=0$,
Fisher information: $I_n= \mathbb{V}[\Lambda]$ \\
\textbf{Rao Cramer bound}: $\E[(\hat{\theta}-\theta_0)^2]\geq \frac{1}{I_n(\theta_0)}$\\
\textbf{General Rao-Cramer bound (biased estimators)}:
$\E_{X|\theta}[(\hat \theta -\theta )^{2}] \geq \frac{[ 1+ \partial (b_{\hat \theta})\partial \theta]^{2}}{\E_{X|\theta}[\Lambda ^{2}]} + b_{\hat \theta}^{2}$ If bias is zero, we recover the previous bound.\\


% Biased estimators: $var(\hat{\theta}) \geq \frac{[1 + b^{\prime}(\theta)]^2}{I(\theta)}$ \\
% Efficiency: $e(\hat{\theta}) = \frac{I(\theta)^{-1}}{var(\hat{\theta})} \leq 1$ \\




