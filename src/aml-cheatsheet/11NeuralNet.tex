\section{Neural Network}

% \textbf{Backpropagation/chain rule}:
% Denote by $\frac{\partial F}{\partial \boldsymbol{w}}$ the Jacobian of $F$:
% $\Big( \frac{\partial F}{\partial \boldsymbol{w}} \Big)_{ij}
% =
% \frac{\partial F_i}{\partial \boldsymbol{w}_j}$.
% Letting $\boldsymbol{w} = (x,y,z)$, $F(x, y) = \alpha$ and $M = G(z, \alpha)$:

% $$
% \frac{\partial M}{\partial y} \bigg{|}_{\boldsymbol{w}} =
% \frac{\partial G}{\partial \alpha} \bigg{|}_{\alpha, z}
% \frac{\partial F}{\partial y} \bigg{|}_{x, y}
% $$

\textbf{Sigmoid}:
$\sigma(x) = \frac{1}{1+\exp(-x)} = \frac{\exp(x)}{\exp(x) + 1}$;
$\sigma(x)' = \sigma(x)(1-\sigma(x)) = \sigma(x)\sigma(-x)$

\textbf{Softmax}: $y_i=\frac{\exp(\beta z_i)}{\sum_{j\leq L}\exp(\beta z_j)}$

\textbf{SGD with Robbins--Monro Algorithm}:
Given $f \colon \R^m \times \R^n \rightarrow \R$, $\b{Z}$ random variable over $\R^m$,
compute $\b{\theta}^*$ s.t.\ $\E_{\b{Z}}[f(\b{Z}, \b{\theta})] = 0$.
Iteratively sample $\b{z}^{(k)} \sim \b{Z}$
and set $\b{\theta}^{(k)} \leftarrow \b{\theta}^{(k-1)} - \eta(k) f(\b{z}_k, \b{\theta}^{(k-1)})$.

For SGD, $f(\b{z}, \b{\theta}) = \nabla_{\b{\theta}} \mathcal{L}(y, \operatorname{NN}_{\b{\theta}}(\b{x}))$.

Thm: R--M (and thus SGD) converges if
$\eta(k) \geq 0$,
$\sum_{k=1}^\infty \eta(k) = \infty$,
$\sum_{k=1}^\infty \eta^2(k) < \infty$
and some regularity conditions on $\E_{\b{Z}}[f(\b{z}, \b{\theta})]$ hold.\\

\textbf{Regularization with NN}: \textbf{weight decay} (small weights decay to zero, large weights are shrunk with constant penalty), \textbf{dropout}, \textbf{optimal brain damage/surgeon} (connections that strongly fluctuate are eliminated).

\textbf{VAE}: define prior over $\mathcal{Z}$, likelihood (decoder) and approx. posterior (encoder). Requirements for good representation:\\
\begin{inparaitem}[\textbullet]
    \item \textbf{informative}: given $\mathcal{Z}$, should be easy to guess $\mathcal{X}$). Encoder function maximizes mutual information: $\theta^* = \argmax_\theta I(\mathcal{X};\mathcal{Z})=\argmax_\theta \E_{\mathcal{X},\mathcal{Z}}[\log\frac{P(\mathcal{X,\mathcal{Z}})}{P(\mathcal{X})P(\mathcal{Z})}]\\
    \approx \argmax_\theta \sum_{i\leq n} \E_{\mathcal{Z}|x_i}[\log P(x_i|\mathcal{Z})]$\\
    \item \textbf{disentangled}: components in $\mathcal{Z}$ associated with distinct feature in $\mathcal{X}$ (see $D_{KL}$ of ELBO). \\
    \item \textbf{robust}: noise in $\mathcal{X}$ does not substantially affect $\mathcal{Z}$ (vice versa). Choice of approx. post.!\\
\end{inparaitem}

\textbf{Training a VAE}: $\argmax_{\theta,\phi}\sum_{i\leqn}\log p_{\theta}(x_i)$\\
\textbf{Derivation of ELBO}:
$
    \log p_{\theta, \phi}(x_i)
    = \E_{Z \sim q_{\phi}(\cdot \mid x_i)}
    \bigg[ \log \Big(
    \frac{p_\theta(x_i, Z)}{p_\theta (Z \mid x_i)}
    \frac{q_\phi(Z \mid x_i)}{q_\phi(Z \mid x_i)}
    \Big) \bigg]
    =
    \underbrace{\E_{\mathcal{Z} \sim q_{\phi}(\cdot \mid x_i) } \big[
        \log p_{\theta}(x_i \mid Z)
    \big]
    - D_{KL} \big(
        q_\phi(\cdot \mid x_i)\ ||\ p(\cdot)
    \big)}_\text{ELBO = Infomax - Regularization term}\\
    + D_{KL} \big(
        q_\phi(\cdot \mid x_i)\ ||\ p_\theta(\cdot \mid x_i)
    \big)
$


